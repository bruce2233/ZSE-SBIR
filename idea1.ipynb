{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from options import Option\n",
    "from data_utils.dataset import load_data_test\n",
    "from model.model import Model\n",
    "from utils.util import setup_seed, load_checkpoint\n",
    "import torchvision\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test args: Namespace(data_path='./datasets', dataset='sketchy_extend', test_class='test_class_sketchy25', cls_number=100, d_model=768, d_ff=1024, head=8, number=1, pretrained=True, anchor_number=49, save='./checkpoints/sketchy_ext', batch=2, epoch=30, datasetLen=10000, learning_rate=1e-05, weight_decay=0.01, pretrain_load=None, load='./checkpoints/sketchy_ext/best_checkpoint.pth', retrieval='rn', testall=False, test_sk=20, test_im=20, num_workers=4, valid_shrink_sk=200, valid_shrink_im=100, choose_cuda='0', seed=2021)\n",
      "current cuda: 0\n"
     ]
    }
   ],
   "source": [
    "# Set arguments of checkpoint and dataset\n",
    "\n",
    "args = Option().parse()\n",
    "args.load = \"./checkpoints/sketchy_ext/best_checkpoint.pth\"\n",
    "args.batch = 2\n",
    "\n",
    "args.valid_shrink_sk=200\n",
    "args.valid_shrink_im=100\n",
    "\n",
    "print(\"test args:\", str(args))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.ap import calculate\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.choose_cuda\n",
    "print(\"current cuda: \" + args.choose_cuda)\n",
    "setup_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used for valid or test sketch / image:\n",
      "(77,) (172,)\n",
      "used for train sketch / image:\n",
      "(55252,) (68401,)\n",
      "=> loading model './checkpoints/sketchy_ext/best_checkpoint.pth'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ff28395a860>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare data\n",
    "sk_valid_data, im_valid_data = load_data_test(args)\n",
    "\n",
    "# prepare model\n",
    "model = Model(args)\n",
    "model = model.half()\n",
    "\n",
    "if args.load is not None:\n",
    "    checkpoint = load_checkpoint(args.load)\n",
    "\n",
    "# load pretrained model\n",
    "cur = model.state_dict()\n",
    "new = {k: v for k, v in checkpoint['model'].items() if k in cur.keys()}\n",
    "cur.update(new)\n",
    "model.load_state_dict(cur)\n",
    "\n",
    "if len(args.choose_cuda) > 1:\n",
    "    model = torch.nn.parallel.DataParallel(model.to('cuda'))\n",
    "model = model.cuda()\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image data\n",
      "loading sketch data\n"
     ]
    }
   ],
   "source": [
    "# load data from validset\n",
    "# 77 sk, 172 im\n",
    "\n",
    "print('loading image data')\n",
    "sk_dataload = DataLoader(sk_valid_data, batch_size=args.test_sk, num_workers=args.num_workers, drop_last=False)\n",
    "print('loading sketch data')\n",
    "im_dataload = DataLoader(im_valid_data, batch_size=args.test_im, num_workers=args.num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:16<00:00,  1.87s/it]\n",
      " 25%|██▌       | 1/4 [00:23<01:11, 23.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 172)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:16<00:00,  1.84s/it]\n",
      " 50%|█████     | 2/4 [00:40<00:39, 19.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 172)\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:16<00:00,  1.85s/it]\n",
      " 75%|███████▌  | 3/4 [00:57<00:18, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 172)\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:14<00:00,  1.58s/it]\n",
      "100%|██████████| 4/4 [01:11<00:00, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 172)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get all sk and im distances, all_dist\n",
    "\n",
    "dist_im = None # distance items in per for loop\n",
    "all_dist = None #(77,172)\n",
    "\n",
    "for i, (sk, sk_label) in enumerate(tqdm(sk_dataload)):\n",
    "        #sk.shape=(20,3,224,224)\n",
    "        print(i)\n",
    "        if i == 0:\n",
    "            all_sk_label = sk_label.numpy()\n",
    "        else:\n",
    "            all_sk_label = np.concatenate((all_sk_label, sk_label.numpy()), axis=0)\n",
    "\n",
    "        sk_len = sk.size(0)\n",
    "        sk = sk.cuda()\n",
    "        #debug\n",
    "        # print(sk[0].shape)\n",
    "        # cv2.imwrite(f\"./logs/sk-{i}\",sk[0].cpu().numpy())\n",
    "        # if i==0:\n",
    "        #     grid_sk = torchvision.utils.make_grid(sk)\n",
    "        #     torchvision.utils.save_image(grid_sk,f\"./logs/sk.jpg\")\n",
    "        \n",
    "        sk, sk_idxs = model(sk, None, 'test', only_sa=True)#sk.shape=(20,192,768)\n",
    "        for j, (im, im_label) in enumerate(tqdm(im_dataload)):\n",
    "            if i == 0 and j == 0:\n",
    "                all_im_label = im_label.numpy()\n",
    "            elif i == 0 and j > 0:\n",
    "                all_im_label = np.concatenate((all_im_label, im_label.numpy()), axis=0)\n",
    "\n",
    "            im_len = im.size(0)\n",
    "            im = im.cuda()\n",
    "            im, im_idxs = model(im, None, 'test', only_sa=True)\n",
    "\n",
    "            sk_temp = sk.unsqueeze(1).repeat(1, im_len, 1, 1).flatten(0, 1).cuda() #(400,197,768) #?difference\n",
    "            im_temp = im.unsqueeze(0).repeat(sk_len, 1, 1, 1).flatten(0, 1).cuda() #(400,197,768)\n",
    "            \n",
    "            if args.retrieval == 'rn':\n",
    "                feature_1, feature_2 = model(sk_temp, im_temp, 'test')\n",
    "            #? when retrieval == 'sa'\n",
    "            if args.retrieval == 'sa':\n",
    "                feature_1, feature_2 = torch.cat((sk_temp[:, 0], im_temp[:, 0]), dim=0), None\n",
    "\n",
    "            # print(feature_1.size())    # [2*sk*im, 768] #2 means sk and im cls\n",
    "            # print(feature_2.size())    # [sk*im, 1]\n",
    "\n",
    "            if args.retrieval == 'rn':\n",
    "                if j == 0:\n",
    "                    dist_im = - feature_2.view(sk_len, im_len).cpu().data.numpy()  # 1*args.batch\n",
    "                else:\n",
    "                    dist_im = np.concatenate((dist_im, - feature_2.view(sk_len, im_len).cpu().data.numpy()), axis=1)\n",
    "            if args.retrieval == 'sa':\n",
    "                dist_temp = F.pairwise_distance(F.normalize(feature_1[:sk_len * im_len]),\n",
    "                                                F.normalize(feature_1[sk_len * im_len:]), 2)\n",
    "                if j == 0:\n",
    "                    dist_im = dist_temp.view(sk_len, im_len).cpu().data.numpy()\n",
    "                else:\n",
    "                    dist_im = np.concatenate((dist_im, dist_temp.view(sk_len, im_len).cpu().data.numpy()), axis=1)\n",
    "\n",
    "        if i == 0:\n",
    "            all_dist = dist_im\n",
    "        else:\n",
    "            all_dist = np.concatenate((all_dist, dist_im), axis=0)\n",
    "        print(all_dist.shape)\n",
    "        #all_dist.shape=(all_sk_label.size, all_im_label.size)\n",
    "    # print(all_sk_label.size, all_im_label.size)     # [762 x 1711] / 2\n",
    "class_same = (np.expand_dims(all_sk_label, axis=1) == np.expand_dims(all_im_label, axis=0)) * 1\n",
    "# print(all_dist.size, class_same.size)     # [762 x 1711] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval time: 0.5534577369689941\n",
      "0.5744169002393182 0.5744169002393182 0.5050711193568337 0.5050711193568337\n"
     ]
    }
   ],
   "source": [
    "# get mAP and Precision\n",
    "\n",
    "np.savetxt(\"./logs/all_dist\",all_dist)\n",
    "np.savetxt(\"./logs/class_same\",class_same)\n",
    "\n",
    "map_all, map_200, precision100, precision200 = calculate(all_dist, class_same, test=True)\n",
    "print(map_all,map_200,precision100,precision200)\n",
    "#0.5744169002393182 0.5744169002393182 0.5050711193568337 0.5050711193568337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 172)\n",
      "[[ 28  29  30 ...  59  58  21]\n",
      " [108  26   0 ...  21  58   6]\n",
      " [  0  30  27 ...   6  58  21]\n",
      " ...\n",
      " [ 16 120 121 ...  42  62   6]\n",
      " [ 20 144 146 ...   6  39   5]\n",
      " [ 77  99 145 ...   1  59  86]]\n"
     ]
    }
   ],
   "source": [
    "# sort all_dist to get the closest images of the sketches\n",
    "\n",
    "arg_sort_sim = all_dist.argsort()   # 得到从小到大索引值\n",
    "print(arg_sort_sim.shape)\n",
    "print(arg_sort_sim)\n",
    "np.savetxt(\"./logs/arg_sort_sim\",torch.tensor(arg_sort_sim,dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch2im(patch_index,im, patch_size):\n",
    "    '''\n",
    "    Get the array of the image by the patch index\n",
    "    \n",
    "    Args:\n",
    "        im: (c, h, w)\n",
    "        patch_index: (2)\n",
    "        patch: int\n",
    "    Returns:\n",
    "        (c, patch_size, patch_size)\n",
    "    '''\n",
    "    # print(patch_index.shape, im.shape, patch_size)\n",
    "    # print(patch_index)\n",
    "    # print(patch_index[0].item()*patch_size)\n",
    "    # print(im.shape)\n",
    "    return im[:, \\\n",
    "        patch_index[0]*patch_size:(patch_index[0].item()+1)*patch_size, \\\n",
    "        patch_index[1].item()*patch_size:(patch_index[1].item()+1)*patch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_replace_data(im_indices, im):\n",
    "    '''\n",
    "    Concatenate the target replaced im embedding by the im_indices\n",
    "    \n",
    "    Args:\n",
    "        im_index: (2) [0]->batch_index, [1]->patch_num_index\n",
    "        im: (b, n, d)\n",
    "    Returns:\n",
    "        im_rtn: (len(im_index), d)\n",
    "    '''\n",
    "    for i, v in enumerate(im_indices):\n",
    "        if i == 0:\n",
    "            print(v)\n",
    "            im_rtn = im[v[0]][v[1]].unsqueeze(0)\n",
    "            print(im_rtn.shape)\n",
    "        else:\n",
    "            im_rtn = torch.cat((im_rtn, im[v[0]][v[1]].unsqueeze(0)))\n",
    "\n",
    "    return im_rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_replace_rgb(im_indices, im, patch_size):\n",
    "    '''\n",
    "    Concatenate the target replaced im embedding by the im_indices\n",
    "    \n",
    "    Args:\n",
    "        im_index: (2) [0]->batch_index, [1]->patch_num_index\n",
    "        im: (b, c, h, w)\n",
    "    Returns:\n",
    "        im_rtn: (len(im_index), d)\n",
    "    '''\n",
    "    \n",
    "    patch_num = im.size(-1) // patch_size\n",
    "    for i, v in enumerate(im_indices):\n",
    "        if i == 0:\n",
    "            print(v, im.shape, im_indices.shape,np.unravel_index(v[1],(patch_num,patch_num)))\n",
    "            im_rtn = patch2im(np.unravel_index(v[1],(patch_num,patch_num)),im[v[0]],patch_size).unsqueeze(0)\n",
    "            print(im_rtn.shape)\n",
    "        else:\n",
    "            im_rtn = torch.cat((im_rtn, patch2im(np.unravel_index(v[1],(patch_num,patch_num)),im[v[0]],patch_size).unsqueeze(0)))\n",
    "\n",
    "    return im_rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_n_closest(sk_index,sort_sim,n):\n",
    "    '''\n",
    "    Args: \n",
    "        sk_index: sk index\n",
    "        sort_sim: sorted similarity matrix,(sk_num, im_num)\n",
    "        n: the returned number of im\n",
    "    Returns:\n",
    "        sk: [1,c,h,w]\n",
    "        im: [n,c,h,w]\n",
    "    '''\n",
    "    im_index = sort_sim[sk_index,:n]\n",
    "    \n",
    "    (sk,_) = sk_valid_data[sk_index]\n",
    "    sk = sk.unsqueeze(0)\n",
    "\n",
    "    tmp = [im_valid_data[i] for i in im_index]\n",
    "    im = [i[0].unsqueeze(0) for i in tmp]\n",
    "    im = torch.concatenate(im)\n",
    "    # print(sk.shape, im.shape)\n",
    "    return sk, im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) torch.Size([3, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([3, 224, 672])\n"
     ]
    }
   ],
   "source": [
    "# get the sk RGB and top 3 im RGB which matches the sk\n",
    "\n",
    "sk_index = 1\n",
    "sk, im = sk_n_closest(sk_index, arg_sort_sim, 3)\n",
    "print(sk.shape, im.shape)\n",
    "\n",
    "torchvision.utils.save_image(sk.cuda(),f\"./logs/sk-{sk_index}.jpg\")\n",
    "\n",
    "im_grid = torchvision.utils.make_grid(im,padding=0)\n",
    "torchvision.utils.save_image(im_grid.cuda(),f\"./logs/im_top_{im.size(0)}_of_{sk_index}.jpg\")\n",
    "print(sk.shape, im_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.float16 cuda:0\n"
     ]
    }
   ],
   "source": [
    "# custom sk and im data\n",
    "\n",
    "# im_index = 0\n",
    "# (sk,_) = sk_valid_data[0]\n",
    "# sk = sk.unsqueeze(0)\n",
    "# (im,_) = im_valid_data[im_index]\n",
    "# im = im.unsqueeze(0)\n",
    "# print(sk.shape,im.shape)\n",
    "\n",
    "# sk = sk.cuda()\n",
    "# im = im.cuda()\n",
    "# print(im.dtype, im.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 197, 768])\n",
      "tensor([[[ 4.9316e-01, -4.1290e-02,  8.8745e-02,  ..., -4.6265e-01,\n",
      "          -5.7129e-01, -2.7441e-01],\n",
      "         [ 3.3081e-01,  2.9810e-01,  1.7529e-01,  ..., -2.1240e-01,\n",
      "          -7.2021e-01, -1.8469e-01],\n",
      "         [ 3.8159e-01,  3.7646e-01,  1.8750e-01,  ..., -2.0508e-01,\n",
      "          -6.6992e-01, -9.9609e-02],\n",
      "         ...,\n",
      "         [ 3.3911e-01,  3.1494e-01,  1.6821e-01,  ..., -1.8494e-01,\n",
      "          -7.1045e-01, -1.5942e-01],\n",
      "         [ 3.5474e-01,  3.3350e-01,  1.5076e-01,  ..., -1.9910e-01,\n",
      "          -6.8750e-01, -1.3074e-01],\n",
      "         [ 3.2739e-01,  3.2153e-01,  1.5869e-01,  ..., -2.4011e-01,\n",
      "          -7.0508e-01, -1.4417e-01]],\n",
      "\n",
      "        [[ 7.6318e-01,  4.7729e-02, -1.0077e-01,  ..., -3.8965e-01,\n",
      "          -4.0723e-01,  7.7332e-02],\n",
      "         [ 5.0342e-01,  1.7859e-01, -1.7236e-01,  ..., -3.3008e-01,\n",
      "          -6.8506e-01, -7.6965e-02],\n",
      "         [ 5.1904e-01,  2.1741e-01, -4.7394e-02,  ..., -4.4385e-01,\n",
      "          -5.8691e-01, -2.1637e-02],\n",
      "         ...,\n",
      "         [ 5.4199e-01,  1.6321e-01,  5.0974e-04,  ..., -3.9282e-01,\n",
      "          -6.5625e-01, -7.9712e-02],\n",
      "         [ 5.4395e-01,  1.9080e-01,  9.4910e-03,  ..., -4.1675e-01,\n",
      "          -6.2891e-01, -6.1798e-02],\n",
      "         [ 5.7275e-01,  1.7236e-01, -1.2421e-02,  ..., -4.5679e-01,\n",
      "          -6.6699e-01, -5.2551e-02]],\n",
      "\n",
      "        [[ 2.8101e-01, -1.5015e-01, -1.3940e-01,  ..., -4.2188e-01,\n",
      "          -6.6992e-01, -5.5634e-02],\n",
      "         [ 1.1993e-01,  2.8052e-01, -8.1299e-02,  ..., -2.1887e-01,\n",
      "          -6.9189e-01, -3.2349e-01],\n",
      "         [ 1.3342e-01,  2.6318e-01, -3.1567e-03,  ..., -2.2070e-01,\n",
      "          -6.2744e-01, -2.6294e-01],\n",
      "         ...,\n",
      "         [ 9.7595e-02,  2.9370e-01,  8.6670e-02,  ..., -1.5625e-01,\n",
      "          -7.0410e-01, -2.4353e-01],\n",
      "         [ 1.3831e-01,  2.8345e-01,  7.0007e-02,  ..., -1.6589e-01,\n",
      "          -6.5918e-01, -2.4548e-01],\n",
      "         [ 8.4717e-02,  3.3765e-01,  5.8685e-02,  ..., -1.5881e-01,\n",
      "          -6.9434e-01, -2.8711e-01]]], device='cuda:0', dtype=torch.float16) <_io.TextIOWrapper name='./logs/im_sa' mode='w' encoding='UTF-8'>\n",
      "tensor([[[ 4.9316e-01, -4.1290e-02,  8.8745e-02,  ..., -4.6265e-01,\n",
      "          -5.7129e-01, -2.7441e-01],\n",
      "         [ 3.3081e-01,  2.9810e-01,  1.7529e-01,  ..., -2.1240e-01,\n",
      "          -7.2021e-01, -1.8469e-01],\n",
      "         [ 3.8159e-01,  3.7646e-01,  1.8750e-01,  ..., -2.0508e-01,\n",
      "          -6.6992e-01, -9.9609e-02],\n",
      "         ...,\n",
      "         [ 3.3911e-01,  3.1494e-01,  1.6821e-01,  ..., -1.8494e-01,\n",
      "          -7.1045e-01, -1.5942e-01],\n",
      "         [ 3.5474e-01,  3.3350e-01,  1.5076e-01,  ..., -1.9910e-01,\n",
      "          -6.8750e-01, -1.3074e-01],\n",
      "         [ 3.2739e-01,  3.2153e-01,  1.5869e-01,  ..., -2.4011e-01,\n",
      "          -7.0508e-01, -1.4417e-01]],\n",
      "\n",
      "        [[ 7.6318e-01,  4.7729e-02, -1.0077e-01,  ..., -3.8965e-01,\n",
      "          -4.0723e-01,  7.7332e-02],\n",
      "         [ 5.0342e-01,  1.7859e-01, -1.7236e-01,  ..., -3.3008e-01,\n",
      "          -6.8506e-01, -7.6965e-02],\n",
      "         [ 5.1904e-01,  2.1741e-01, -4.7394e-02,  ..., -4.4385e-01,\n",
      "          -5.8691e-01, -2.1637e-02],\n",
      "         ...,\n",
      "         [ 5.4199e-01,  1.6321e-01,  5.0974e-04,  ..., -3.9282e-01,\n",
      "          -6.5625e-01, -7.9712e-02],\n",
      "         [ 5.4395e-01,  1.9080e-01,  9.4910e-03,  ..., -4.1675e-01,\n",
      "          -6.2891e-01, -6.1798e-02],\n",
      "         [ 5.7275e-01,  1.7236e-01, -1.2421e-02,  ..., -4.5679e-01,\n",
      "          -6.6699e-01, -5.2551e-02]],\n",
      "\n",
      "        [[ 2.8101e-01, -1.5015e-01, -1.3940e-01,  ..., -4.2188e-01,\n",
      "          -6.6992e-01, -5.5634e-02],\n",
      "         [ 1.1993e-01,  2.8052e-01, -8.1299e-02,  ..., -2.1887e-01,\n",
      "          -6.9189e-01, -3.2349e-01],\n",
      "         [ 1.3342e-01,  2.6318e-01, -3.1567e-03,  ..., -2.2070e-01,\n",
      "          -6.2744e-01, -2.6294e-01],\n",
      "         ...,\n",
      "         [ 9.7595e-02,  2.9370e-01,  8.6670e-02,  ..., -1.5625e-01,\n",
      "          -7.0410e-01, -2.4353e-01],\n",
      "         [ 1.3831e-01,  2.8345e-01,  7.0007e-02,  ..., -1.6589e-01,\n",
      "          -6.5918e-01, -2.4548e-01],\n",
      "         [ 8.4717e-02,  3.3765e-01,  5.8685e-02,  ..., -1.5881e-01,\n",
      "          -6.9434e-01, -2.8711e-01]]], device='cuda:0', dtype=torch.float16)\n",
      "torch.Size([4, 196, 768])\n",
      "torch.Size([4, 768, 14, 14])\n",
      "torch.Size([4, 256, 32, 32])\n",
      "torch.Size([4, 1024, 256])\n"
     ]
    }
   ],
   "source": [
    "#embed sk and im\n",
    "\n",
    "from model import rn\n",
    "\n",
    "#sa\n",
    "sk_sa, sk_idxs = model(sk.cuda(), None, 'test', only_sa=True)#sk_sa.shape=(20,192,768)\n",
    "im_sa, im_idxs = model(im.cuda(), None, 'test', only_sa=True)#im_sa.shape=(20,192,768)\n",
    "sk_im_sa = torch.cat((sk_sa, im_sa), dim=0)\n",
    "print(sk_im_sa.shape)\n",
    "print(im_sa, open(\"./logs/im_sa\",\"w\"))\n",
    "print(im_sa)\n",
    "\n",
    "#ca\n",
    "ca_fea = model.ca(sk_im_sa)  # [2b, 197, 768]\n",
    "cls_fea = ca_fea[:, 0]  # [2b, 1, 768]\n",
    "token_fea = ca_fea[:, 1:]  # [2b, 196, 768]\n",
    "print(token_fea.shape)\n",
    "\n",
    "\n",
    "token_fea_tmp = einops.rearrange(token_fea, \"b (h w) c -> b c h w\", h=14)\n",
    "print(token_fea_tmp.shape) #(b, 768, 14,14)\n",
    "up_fea = model.output4VQGAN(token_fea_tmp)\n",
    "print(up_fea.shape) #(b, 256,32,32)\n",
    "up_fea = einops.rearrange(up_fea, \"b c h w -> b (h w) c\")\n",
    "print(up_fea.shape) #(b, 1024, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 14, 14]) torch.Size([3, 768, 14, 14])\n",
      "torch.Size([1, 196, 768]) torch.Size([3, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "# select patch_size 7-32*32 or 16-14*14\n",
    "\n",
    "# sk_fea = up_fea[0]\n",
    "# im_fea = up_fea[sk.size(0):]\n",
    "\n",
    "sk_fea = token_fea_tmp[0]\n",
    "sk_fea = sk_fea.unsqueeze(0)\n",
    "im_fea = token_fea_tmp[sk.size(0):]\n",
    "print(sk_fea.shape,im_fea.shape)\n",
    "\n",
    "sk_fea = einops.rearrange(sk_fea, \"b c h w -> b (h w) c\")\n",
    "im_fea = einops.rearrange(im_fea, \"b c h w -> b (h w) c\")\n",
    "\n",
    "print(sk_fea.shape,im_fea.shape) #(b, 1024, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768]) torch.Size([3, 196, 768])\n",
      "torch.Size([3, 196, 196])\n"
     ]
    }
   ],
   "source": [
    "# calculate the cos_scores between sk_fea and n im_fea\n",
    "# cos_scores (n,1024,1024)\n",
    "\n",
    "# np.savetxt(\"./logs/sk_fea\", sk_fea.cpu())\n",
    "# np.savetxt(\"./logs/im_fea\", im_fea.cpu())\n",
    "print(sk_fea.shape, im_fea.shape)\n",
    "cos_scores = rn.cos_similar(sk_fea, im_fea)\n",
    "print(cos_scores.shape)\n",
    "np.savetxt(\"./logs/cos_scores\",cos_scores.cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7505, 0.7676, 0.7593,  ..., 0.6045, 0.6123, 0.5986],\n",
      "        [0.7598, 0.7744, 0.7681,  ..., 0.6001, 0.6064, 0.5957],\n",
      "        [0.7612, 0.7695, 0.7671,  ..., 0.5977, 0.6011, 0.5938],\n",
      "        ...,\n",
      "        [0.7544, 0.7646, 0.7607,  ..., 0.6001, 0.6045, 0.5942],\n",
      "        [0.7476, 0.7637, 0.7563,  ..., 0.6045, 0.6113, 0.5986],\n",
      "        [0.7314, 0.7534, 0.7432,  ..., 0.6084, 0.6162, 0.6035]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "torch.Size([196, 2])\n"
     ]
    }
   ],
   "source": [
    "# get the max_indices of im_fea to be replaced\n",
    "\n",
    "max_indices = torch.empty((0,2), dtype=int)\n",
    "\n",
    "b = einops.rearrange(cos_scores,\"n s i -> s (n i)\") # 1024 x 3072\n",
    "print(b)\n",
    "\n",
    "\n",
    "for i in b:\n",
    "    max_indices_item = torch.argmax(i)\n",
    "    # print(i.shape)\n",
    "    new = np.unravel_index(max_indices_item.cpu(),(cos_scores.shape[0],cos_scores.shape[2]))\n",
    "    # print(torch.Tensor(new))\n",
    "    max_indices = torch.cat((max_indices, torch.tensor(new, dtype=torch.int).unsqueeze(0)), 0)\n",
    "    # print(max_indices)\n",
    "    \n",
    "# print(np.unravel_index(b.values, (3, 196)))\n",
    "np.savetxt(\"./logs/max_indices\",max_indices)\n",
    "print(max_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 14])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([196, 768])\n"
     ]
    }
   ],
   "source": [
    "# replace the sk embeddings with im embeddings\n",
    "\n",
    "# im_replaced = patch_replace_data(max_indices, im_fea)\n",
    "# print(im_replaced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 224, 224])\n",
      "torch.Size([196, 2]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(im.shape)\n",
    "# im = im.squeeze()\n",
    "print(max_indices.shape, max_indices.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 14]) torch.Size([3, 3, 224, 224]) torch.Size([196, 2]) (1, 0)\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 16, 16])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([196, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "im_replaced = patch_replace_rgb(max_indices, im,16)\n",
    "print(im_replaced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "im_replaced = torchvision.utils.make_grid(im_replaced,nrow=14,padding=0)\n",
    "print(im_replaced.shape)\n",
    "torchvision.utils.save_image(im_replaced.cuda(),\"./logs/patch_replace_14_top3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test patch_replace_data function\n",
    "# test = torch.arange(0,3*1024).reshape(3,1024,1)\n",
    "# im_replaced_test = patch_replace_data(max_indices, test)\n",
    "# print(im_replaced_test, file=open(\"logs/im_replaced_rgb\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure vqgan model\n",
    "\n",
    "import torch\n",
    "vqgan_dict = torch.load(\"../download/last.ckpt\")\n",
    "\n",
    "# !proxychains git clone https://github.com/CompVis/taming-transformers\n",
    "# %cd taming-transformers\n",
    "\n",
    "# !mkdir -p logs/vqgan_imagenet_f16_1024/checkpoints\n",
    "# !mkdir -p logs/vqgan_imagenet_f16_1024/configs\n",
    "# # !wget 'https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1' -O 'logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt' \n",
    "# !proxychains wget 'https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1' -O 'taming-transformers/logs/vqgan_imagenet_f16_1024/configs/model.yaml' \n",
    "\n",
    "# %pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops>=0.3.0\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"taming-transformers/\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vqgan load function and scripts\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "  config = OmegaConf.load(config_path)\n",
    "  if display:\n",
    "    print(yaml.dump(OmegaConf.to_container(config)))\n",
    "  return config\n",
    "\n",
    "def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n",
    "  if is_gumbel:\n",
    "    model = GumbelVQ(**config.model.params)\n",
    "  else:\n",
    "    model = VQModel(**config.model.params)\n",
    "  if ckpt_path is not None:\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "  return model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vqgan model as model1024\n",
    "\n",
    "config1024 = load_config(\"taming-transformers/logs/vqgan_imagenet_f16_1024/configs/model.yaml\", display=False)\n",
    "model1024 = load_vqgan(config1024, ckpt_path=\"taming-transformers/logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt\").to(DEVICE)\n",
    "\n",
    "print(model1024)\n",
    "print(model1024, file=open(\"logs/model1024_info\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the im_replaced up_fea to match the input of vqgan quantize\n",
    "\n",
    "h = einops.rearrange(im_fea,\"b (h w) c -> b c h w\",h=32) #(1,256,1024)\n",
    "h = h.to(torch.float32)\n",
    "h = h *10\n",
    "# h = h.flatten()\n",
    "print(h, file=open(\"./logs/h\",\"w\"))\n",
    "print(h.shape)\n",
    "print(h.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize\n",
    "\n",
    "quant, emb_loss, info = model1024.quantize(h) #don't use same name\n",
    "print(quant, emb_loss, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode\n",
    "\n",
    "dec = model1024.decoder(quant)\n",
    "print(dec.shape)\n",
    "torchvision.utils.save_image(dec,\"logs/dec.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model1024\n",
    "\n",
    "im_index = 28\n",
    "(sk,_) = sk_valid_data[0]\n",
    "(im,_) = im_valid_data[im_index]\n",
    "im = im.unsqueeze(0)\n",
    "print(sk.shape,im.shape)\n",
    "\n",
    "im = im.to(torch.float32).cuda()\n",
    "print(im.dtype, im.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = im.to(torch.float32)\n",
    "quant, emb_loss, info = model1024.encode(im)\n",
    "\n",
    "h = model1024.encoder(im)\n",
    "h = model1024.quant_conv(h)\n",
    "print(h,file=open(\"./logs/im_embeded\",\"w\"))\n",
    "\n",
    "dec = model1024.decode(quant)\n",
    "\n",
    "print(im_index)\n",
    "torchvision.utils.save_image(im,f\"./logs/im_test{im_index}_2.jpg\")\n",
    "torchvision.utils.save_image(dec,f\"./logs/dec_test{im_index}_2.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4818a0b8c316263be072c2082609790d2bac6bbfe2378382b84905edb944ba2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
