{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from options import Option\n",
    "from data_utils.dataset import load_data_test\n",
    "from model.model import Model\n",
    "from utils.util import setup_seed, load_checkpoint\n",
    "import torchvision\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Option().parse()\n",
    "args.load = \"./checkpoints/sketchy_ext/best_checkpoint.pth\"\n",
    "args.batch = 2\n",
    "args.valid_shrink_sk=6000\n",
    "args.valid_shrink_im=10\n",
    "\n",
    "print(\"test args:\", str(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.ap import calculate\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.choose_cuda\n",
    "print(\"current cuda: \" + args.choose_cuda)\n",
    "setup_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "sk_valid_data, im_valid_data = load_data_test(args)\n",
    "\n",
    "# prepare model\n",
    "model = Model(args)\n",
    "model = model.half()\n",
    "\n",
    "if args.load is not None:\n",
    "    checkpoint = load_checkpoint(args.load)\n",
    "\n",
    "cur = model.state_dict()\n",
    "new = {k: v for k, v in checkpoint['model'].items() if k in cur.keys()}\n",
    "cur.update(new)\n",
    "model.load_state_dict(cur)\n",
    "\n",
    "if len(args.choose_cuda) > 1:\n",
    "    model = torch.nn.parallel.DataParallel(model.to('cuda'))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "print('loading image data')\n",
    "sk_dataload = DataLoader(sk_valid_data, batch_size=args.test_sk, num_workers=args.num_workers, drop_last=False)\n",
    "print('loading sketch data')\n",
    "im_dataload = DataLoader(im_valid_data, batch_size=args.test_im, num_workers=args.num_workers, drop_last=False)\n",
    "\n",
    "dist_im = None\n",
    "all_dist = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (sk, sk_label) in enumerate(tqdm(sk_dataload)):\n",
    "        #sk.shape=(20,3,224,224)\n",
    "        print(i)\n",
    "        if i == 0:\n",
    "            all_sk_label = sk_label.numpy()\n",
    "        else:\n",
    "            all_sk_label = np.concatenate((all_sk_label, sk_label.numpy()), axis=0)\n",
    "\n",
    "        sk_len = sk.size(0)\n",
    "        sk = sk.cuda()\n",
    "        #debug\n",
    "        print(sk[0].shape)\n",
    "        # cv2.imwrite(f\"./logs/sk-{i}\",sk[0].cpu().numpy())\n",
    "        if i==0:\n",
    "            grid_sk = torchvision.utils.make_grid(sk)\n",
    "            torchvision.utils.save_image(grid_sk,f\"./logs/sk.jpg\")\n",
    "        \n",
    "        sk, sk_idxs = model(sk, None, 'test', only_sa=True)#sk.shape=(20,192,768)\n",
    "        for j, (im, im_label) in enumerate(tqdm(im_dataload)):\n",
    "            if i == 0 and j == 0:\n",
    "                all_im_label = im_label.numpy()\n",
    "            elif i == 0 and j > 0:\n",
    "                all_im_label = np.concatenate((all_im_label, im_label.numpy()), axis=0)\n",
    "\n",
    "            im_len = im.size(0)\n",
    "            im = im.cuda()\n",
    "            im, im_idxs = model(im, None, 'test', only_sa=True)\n",
    "\n",
    "            sk_temp = sk.unsqueeze(1).repeat(1, im_len, 1, 1).flatten(0, 1).cuda() #(400,197,768) #?difference\n",
    "            im_temp = im.unsqueeze(0).repeat(sk_len, 1, 1, 1).flatten(0, 1).cuda() #(400,197,768)\n",
    "            \n",
    "            if args.retrieval == 'rn':\n",
    "                feature_1, feature_2 = model(sk_temp, im_temp, 'test')\n",
    "            #? when retrieval == 'sa'\n",
    "            if args.retrieval == 'sa':\n",
    "                feature_1, feature_2 = torch.cat((sk_temp[:, 0], im_temp[:, 0]), dim=0), None\n",
    "\n",
    "            # print(feature_1.size())    # [2*sk*im, 768] #2 means sk and im cls\n",
    "            # print(feature_2.size())    # [sk*im, 1]\n",
    "\n",
    "            if args.retrieval == 'rn':\n",
    "                if j == 0:\n",
    "                    dist_im = - feature_2.view(sk_len, im_len).cpu().data.numpy()  # 1*args.batch\n",
    "                else:\n",
    "                    dist_im = np.concatenate((dist_im, - feature_2.view(sk_len, im_len).cpu().data.numpy()), axis=1)\n",
    "            if args.retrieval == 'sa':\n",
    "                dist_temp = F.pairwise_distance(F.normalize(feature_1[:sk_len * im_len]),\n",
    "                                                F.normalize(feature_1[sk_len * im_len:]), 2)\n",
    "                if j == 0:\n",
    "                    dist_im = dist_temp.view(sk_len, im_len).cpu().data.numpy()\n",
    "                else:\n",
    "                    dist_im = np.concatenate((dist_im, dist_temp.view(sk_len, im_len).cpu().data.numpy()), axis=1)\n",
    "\n",
    "        if i == 0:\n",
    "            all_dist = dist_im\n",
    "        else:\n",
    "            all_dist = np.concatenate((all_dist, dist_im), axis=0)\n",
    "        print(all_dist.shape)\n",
    "        #all_dist.shape=(all_sk_label.size, all_im_label.size)\n",
    "    # print(all_sk_label.size, all_im_label.size)     # [762 x 1711] / 2\n",
    "class_same = (np.expand_dims(all_sk_label, axis=1) == np.expand_dims(all_im_label, axis=0)) * 1\n",
    "# print(all_dist.size, class_same.size)     # [762 x 1711] / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_same.shape)\n",
    "print(class_same)\n",
    "np.savetxt(\"./logs/all_dist\",all_dist)\n",
    "np.savetxt(\"./logs/class_same\",class_same)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_all, map_200, precision100, precision200 = calculate(all_dist, class_same, test=True)\n",
    "print(map_all,map_200,precision100,precision200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_sort_sim = all_dist.argsort()   # 得到从小到大索引值\n",
    "print(arg_sort_sim.shape)\n",
    "print(arg_sort_sim)\n",
    "np.savetxt(\"./logs/arg_sort_sim\",torch.tensor(arg_sort_sim,dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch2im(patch_index,im, patch_size=16):\n",
    "    '''\n",
    "    im: (c, w, h)\n",
    "    patch_index: (2)\n",
    "    return: (c, patch_size, patch_size)\n",
    "    '''\n",
    "    # print(patch_index.shape, im.shape, patch_size)\n",
    "    # print(patch_index)\n",
    "    # print(patch_index[0].item()*patch_size)\n",
    "        \n",
    "    return im[:, \\\n",
    "        patch_index[0]*patch_size:(patch_index[0]+1)*patch_size, \\\n",
    "        patch_index[1]*patch_size:(patch_index[1]+1)*patch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_match(im, indices,patch_size=16):\n",
    "    '''\n",
    "        im: (b,c,w,h)\n",
    "        indices: (m,im.shape.len)\n",
    "    '''\n",
    "    # print(im.shape)\n",
    "    # x = torch.zeros((0,)+tuple(im.shape[1:]))\n",
    "    # print(x)\n",
    "    x = None\n",
    "    for i in indices:\n",
    "        patch_index = np.unravel_index(i[1],(im.size(-1)//patch_size,im.size(-1)//patch_size))\n",
    "        item = patch2im(patch_index, im[i[0]], patch_size).unsqueeze(0)\n",
    "        # print(item.shape)\n",
    "        if x is None:\n",
    "            x=item\n",
    "        else:\n",
    "            x= torch.cat([x, item])\n",
    "    return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_replace_data(im_index,im, patch_size=16):\n",
    "    '''\n",
    "    create an image from the image patch index\n",
    "    \n",
    "    im_index: (2), [=b_i, =n_i]\n",
    "    im: (b,n), [b, n, ......]\n",
    "    '''\n",
    "    \n",
    "    for i,v in enumerate(im_index):\n",
    "        if i == 0:    \n",
    "            # print(v)\n",
    "            im_rtn = patch2im(v, im, patch_size)\n",
    "            # print(im_rtn.shape)\n",
    "        else:    \n",
    "            im_rtn = torch.cat([im_rtn, patch2im(v,im,patch_size)])\n",
    "    return im_rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (patch_replaced.py, line 53)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/home/bruce/miniconda3/envs/taming/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3505\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 1\u001b[0;36m\n\u001b[0;31m    import data_utils.patch_replaced\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/app/ZSE-SBIR/data_utils/patch_replaced.py:53\u001b[0;36m\u001b[0m\n\u001b[0;31m    def\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import data_utils.patch_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sk_im():\n",
    "    sk_index= 2\n",
    "    im_index = arg_sort_sim[sk_index,:1]\n",
    "    return sk_index, im_index\n",
    "# (sk_tmp, im_tmp) = patch_replace_data(max_indices, im_valid_data[im_index[0],im_index[1],im_index[2],])\n",
    "\n",
    "sk_index, im_index = select_sk_im()\n",
    "print(sk_index, im_index)\n",
    "(sk,_) = sk_valid_data[sk_index]\n",
    "sk = sk.unsqueeze(0)\n",
    "\n",
    "tmp = [im_valid_data[i] for i in im_index]\n",
    "im = [i[0].unsqueeze(0) for i in tmp]\n",
    "im = torch.cat(im)\n",
    "print(sk.shape, im.shape)\n",
    "\n",
    "torchvision.utils.save_image(sk.cuda(),f\"./logs/sk-{sk_index}.jpg\")\n",
    "\n",
    "im_tmp = torchvision.utils.make_grid(im)\n",
    "torchvision.utils.save_image(im_tmp.cuda(),f\"./logs/im_{sk_index}_top_{len(im_index)}.jpg\")\n",
    "print(sk.shape, im_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import rn\n",
    "\n",
    "\n",
    "print(sk.shape, im.shape)\n",
    "\n",
    "sk_sa, sk_idxs = model(sk.cuda(), None, 'test', only_sa=True)#sk_sa.shape=(20,192,768)\n",
    "im_sa, im_idxs = model(im.cuda(), None, 'test', only_sa=True)#im_sa.shape=(20,192,768)\n",
    "\n",
    "\n",
    "sk_im_sa = torch.cat((sk_sa, im_sa), dim=0)\n",
    "print(sk_im_sa.shape)\n",
    "ca_fea = model.ca(sk_im_sa)  # [2b, 197, 768]\n",
    "cls_fea = ca_fea[:, 0]  # [2b, 1, 768]\n",
    "token_fea = ca_fea[:, 1:]  # [2b, 196, 768]\n",
    "print(token_fea.shape)\n",
    "\n",
    "token_fea_tmp = einops.rearrange(token_fea, \"b (h w) c -> b c h w\", h=14)\n",
    "print(token_fea_tmp.shape)\n",
    "up_fea = model.output4VQGAN(token_fea_tmp)\n",
    "print(up_fea.shape)\n",
    "up_fea = einops.rearrange(up_fea, \"b c h w -> b (h w) c\")\n",
    "print(up_fea.shape)\n",
    "\n",
    "batch = token_fea.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_fea = einops.rearrange(token_fea,\"b d h w -> b d (h w)\") #token_fea = token_fea.view(batch, 768, 14, 14)\n",
    "def fea_process(sk_fea, im_fea, upsample=None):\n",
    "    if upsample is not None:\n",
    "        return upsample(sk_fea, im_fea)\n",
    "    return sk_fea, im_fea\n",
    "\n",
    "cos_scores = \n",
    "sk_fea, im_fea = fea_process(token_fea[sk.size(0)-1], token_fea[sk.size(0):])\n",
    "print(sk_fea.shape, im_fea.shape)\n",
    "cos_scores = rn.cos_similar(sk_fea, im_fea)\n",
    "print(cos_scores.shape)\n",
    "np.savetxt(\"./logs/cos_scores\",cos_scores.cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cos_scores.argsort(0).shape,cos_scores.argsort(0))\n",
    "# print(torch.argmax(einops.rearrange(cos_scores,\"a b c -> b (a c)\")))\n",
    "b = einops.rearrange(cos_scores,\"a b c -> b (a c)\")\n",
    "# print(cos_scores.shape,cos_scores)\n",
    "\n",
    "max_indices = torch.empty((0,2), dtype=int)\n",
    "print(b)\n",
    "print(max_indices)\n",
    "\n",
    "for i in b:\n",
    "    max_indices_item = torch.argmax(i)\n",
    "    # print(i.shape)\n",
    "    new = np.unravel_index(max_indices_item.cpu(),(cos_scores.shape[0],cos_scores.shape[2]))\n",
    "    # print(torch.Tensor(new))\n",
    "    max_indices = torch.cat((max_indices, torch.tensor(new, dtype=torch.int).unsqueeze(0)), 0)\n",
    "    # print(max_indices)\n",
    "    \n",
    "# print(np.unravel_index(b.values, (3, 196)))\n",
    "np.savetxt(\"./logs/max_indices\",max_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_indices.shape, im.shape)\n",
    "im_replaced = patch_match(im,max_indices,16)\n",
    "print(im_replaced.shape)\n",
    "\n",
    "im_replaced = torchvision.utils.make_grid(im_replaced,14,padding=0).to(\"cuda\")\n",
    "print(im_replaced.shape)\n",
    "\n",
    "im_replaced_sketch = torch.cat([im_replaced.unsqueeze(0),(sk[0].unsqueeze(0).to(\"cuda\"))])\n",
    "print(im_replaced_sketch.shape)\n",
    "im_replaced_sketch = torchvision.utils.make_grid(im_replaced_sketch)\n",
    "\n",
    "torchvision.utils.save_image(im_replaced_sketch,f\"logs/replaced{sk_index}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch replace op test\n",
    "# indices = max_indices\n",
    "\n",
    "# print(im.shape)\n",
    "# # x = torch.zeros((0,)+tuple(im.shape[1:]))\n",
    "# x = torch.zeros((0, 3, 14,14))\n",
    "# # print(x)\n",
    "# for i,v in enumerate(indices):\n",
    "#     patch_index = np.unravel_index(i,(16,16))\n",
    "#     item = patch2im(torch.tensor(patch_index,dtype=int), im[0], im.shape[-1]//16)\n",
    "#     # print(item.shape)\n",
    "#     x= torch.cat([x, item.unsqueeze(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_indices.shape,up_fea.shape)\n",
    "im_replaced = patch_replace_data(max_indices, im_fea)\n",
    "print(im_replaced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = max_indices\n",
    "\n",
    "# print(im.shape)\n",
    "# # x = torch.zeros((0,)+tuple(im.shape[1:]))\n",
    "# x = torch.zeros((0, 3, 16,16))\n",
    "# # print(x)\n",
    "# for i in indices:\n",
    "#     patch_index = np.unravel_index(i[1],(14,14))\n",
    "#     item = patch2im(torch.tensor(patch_index,dtype=int), im[i[0]], int(im.shape[-1]/14))\n",
    "#     # print(item.shape)\n",
    "#     x= torch.cat([x, item.unsqueeze(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torchvision.utils.make_grid(x,nrow=14)\n",
    "# torchvision.utils.save_image(x,\"./logs/patch_replace.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid\n",
    "# map_all, map_200, precision_100, precision_200 = valid_cls(args, model, sk_valid_data, im_valid_data)\n",
    "print(f'map_all:{map_all:.4f} map_200:{map_200:.4f} precision_100:{precision100:.4f} precision_200:{precision200:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vqgan_dict = torch.load(\"../download/last.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!proxychains git clone https://github.com/CompVis/taming-transformers\n",
    "%cd taming-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p logs/vqgan_imagenet_f16_1024/checkpoints\n",
    "!mkdir -p logs/vqgan_imagenet_f16_1024/configs\n",
    "# !wget 'https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1' -O 'logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt' \n",
    "!proxychains wget 'https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1' -O 'taming-transformers/logs/vqgan_imagenet_f16_1024/configs/model.yaml' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops>=0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"taming-transformers/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "  config = OmegaConf.load(config_path)\n",
    "  if display:\n",
    "    print(yaml.dump(OmegaConf.to_container(config)))\n",
    "  return config\n",
    "\n",
    "def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n",
    "  if is_gumbel:\n",
    "    model = GumbelVQ(**config.model.params)\n",
    "  else:\n",
    "    model = VQModel(**config.model.params)\n",
    "  if ckpt_path is not None:\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "  return model.eval()\n",
    "\n",
    "def preprocess_vqgan(x):\n",
    "  x = 2.*x - 1.\n",
    "  return x\n",
    "\n",
    "def custom_to_pil(x):\n",
    "  x = x.detach().cpu()\n",
    "  x = torch.clamp(x, -1., 1.)\n",
    "  x = (x + 1.)/2.\n",
    "  x = x.permute(1,2,0).numpy()\n",
    "  x = (255*x).astype(np.uint8)\n",
    "  x = Image.fromarray(x)\n",
    "  if not x.mode == \"RGB\":\n",
    "    x = x.convert(\"RGB\")\n",
    "  return x\n",
    "\n",
    "def reconstruct_with_vqgan(x, model):\n",
    "  # could also use model(x) for reconstruction but use explicit encoding and decoding here\n",
    "  z, _, [_, _, indices] = model.encode(x)\n",
    "  print(f\"VQGAN --- {model.__class__.__name__}: latent shape: {z.shape[2:]}\")\n",
    "  xrec = model.decode(z)\n",
    "  return xrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1024 = load_config(\"taming-transformers/logs/vqgan_imagenet_f16_1024/configs/model.yaml\", display=False)\n",
    "model1024 = load_vqgan(config1024, ckpt_path=\"taming-transformers/logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt\").to(DEVICE)\n",
    "\n",
    "print(model1024)\n",
    "print(model1024, file=open(\"logs/model1024_info\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xrec  = reconstruct_with_vqgan(x, model1024)\n",
    "h = einops.rearrange(im_replaced.unsqueeze(0),\"b (h w) c -> b c h w\",h=32) #(1,256,1024)\n",
    "h = h.to(torch.float32)\n",
    "\n",
    "# h = h.flatten()\n",
    "print(h, file=open(\"./logs/h\",\"w\"))\n",
    "h = einops.rearrange(h,\"(b c h w) -> b c h w\",b=1,c=256,h=32) #(1,256,1024)\n",
    "\n",
    "print(h.shape)\n",
    "print(h.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = einops.rearrange(h, 'b c h w -> b h w c').contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "h = torch.arange(0.,1024*256.).reshape(1,32,32,256).cuda()\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model1024.quantize.forward(h) #don't use same name\n",
    "print(model1024.quantize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant, emb_loss, info = model1024.quantize(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4818a0b8c316263be072c2082609790d2bac6bbfe2378382b84905edb944ba2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
